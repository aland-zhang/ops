腾讯业务立体化监控体系

一、概述
嘉宾介绍：罗伟，2008年加入腾讯，长期从事腾讯SNS业务的运营支撑、监控系统的建设，曾主导建设Qzone、手机Qzone、手机QQ等业务监控系统及监控平台的建立和运营。目前负责腾讯云监控平台的建设工作。 
分享主题：腾讯业务立体化监控体系： 
1、介绍腾讯业务监控体系的层级构成； 
2、用代表性的监控系统阐述每个监控层次的实现方法；
3、与监控体系配合，业务做了哪些容灾和调度的方案；

腾讯的业务从逻辑上可以抽象总结成四个层次：用户层（PC、手机、智能硬件等）、接入层、逻辑层和数据层（包括数据缓存层和持久化层）。
腾讯业务的监控系统是立体化覆盖，总结来说也是由四个层级组成：

二、基础设施层：
基础设施层的监控覆盖范围很广，在腾讯包括：与运营商互联出口、专线(包括城域和广域)、机房(包括各类物理设施---如机架、制冷、配电、消防、安防等)、网络设备（交换机、路由器、防火墙等）等。
基础设施层的监控又分为状态、性能、质量、容量、架构等几个层面。举例说明，
状态监控包括网络设备的软硬件状态（如设备存活状态、板卡、电源、风扇状态，设备温度、光功率、OSPF状态、生成树状态等）；
性能监控包括设备CPU、设备内存大小、session数量、端口流量包量、内存溢出监控、内存使用率等；
质量监控包括设备错包、丢包率，针对网络设备以及网络链路的探测延时、丢包率监控等；
容量监控包括设备负载使用率、专线带宽使用率、出口流量分布等；
架构监控包括路由跳变、缺失、绕行，流量穿越监控等。


三、服务器层：
服务器是业务部署运行起来的载体（早期服务器就是我们传统观念上的“物理机+操作系统”，现在已经扩大到虚拟机或者是容器等范畴）。服务器层的监控包括硬件层面和软件层面。

硬件层面的监控主要包括如下内容，
硬盘：硬盘读写错误、读写超时、硬盘掉线、硬盘介质错误、[SSD硬盘]硬盘温度、硬盘寿命、硬盘坏块率；
内存：内存缺失、内存配置错误、内存不可用、内存校验；
网卡：网卡速率；电源：电源电压、电源模块是否失效；
风扇：风扇转速；
Raid卡：Raid卡电池状态、电池老化、电池和缓存是否在位、缓存策略。

软件层面的监控主要包括：CPU（CPU整体使用率、CPU各核使用率、CPU Load负载）、内存（应用内存、整体内存、Swap等）、磁盘IO（读写速率、IOPS、平均等待延时、平均服务延时等）、网络IO（流量、包量、错包、丢包）、连接（各种状态的TCP连接数等）、进程端口存活、文件句柄数、进程数、内网探测延时、丢包率等。


四、业务程序层：
业务程序层的监控手段是最为丰富和立体化的，这里只是介绍几个核心和典型的系统。

【容量管理系统】
容量管理系统基于“服务器层”在软件层面的监控指标，并且配合业务增长、运营活动等因素而建设，用于客观衡量业务负载高低情况，并结合扩缩容调度，实现业务的负载和成本间的平衡。
具体原理是根据服务器所在业务层级（接入层、逻辑层还是数据层）的不同，设置不同的容量参考指标、指标参考基准、指标计算规则、高低负载判别规则，设置业务模块（由相同功能的多个服务器构成的业务集群）的扩缩容规则；由系统计算出服务器、业务模块的负载情况，决策出是否需要扩容或缩容，触发业务模块的扩缩容操作。
说明：服务器、业务模块的负载计算规则也是由业务可以自定义配置。

【模块间调用】
在腾讯内部简称“模调”，2006年开始已经广泛应用于各大业务，用于实时监测后端服务与服务之间调用的质量，可以细化到服务模块、接口、命令字甚至代码层面（现在看来，其实就是各个APM厂商在大力宣传和推广的产品）。
1、针对使用标准化组件（在腾讯内部业务，用户层使用的标准组件是wns；接入层使用的标准组件是Qzhttp、tngix；逻辑层使用的标准组件是spp+L5；数据层使用的标准组件是CKV、CDB等）的业务，由标准组件上报模调监控数据；
2、针对自定义业务Server，提供模调上报的SDK或API，由业务自主上报服务间的每次调用成功与否，每次调用的延时；
3、模调系统支持业务从用户层->接入层->逻辑层->数据层，全路径用唯一的序列号(通常由时间、功能模块ID、UIN、随机值等因素构成此值)来对业务请求染色，方便业务展现出每次请求完整的从前到后的调用链路。


五、用户体验

测速系统：
收集用户真实访问业务的速度、性能、成功率数据。PC类业务由js上报或者客户端程序监控模块上报，移动类业务通过引入腾讯分析SDK上报到监控系统。测速系统的价值不仅仅在于实时监控，还有一个比较有意义的作用

云拨测：
通过模拟用户访问业务并校验返回数据结果，监测业务是否可用、访问质量及性能、逻辑功能正确性的监控系统。
当然和云拨测同类的产品或者公司也挺多的，比如基调、监控宝等等。我们自己要建立云拨测其中的一个原因是：
腾讯业务需要监控业务逻辑是否正常，而不仅仅是接入层(网站类业务是否能访问，访问的速度是否快)，业务逻辑的验证就涉及到鉴权、关系数据的自动化等，外部监控服务商无法实现这一点。
以上的内容简单介绍了目前腾讯业务核心的几个监控系统，当然还有很多其他系统没有提及到，比如自动化测试监控、组件特性监控、业务染色等。
接下来讲下告警关联和业务容灾的内容


六、告警智能关联
有这么多监控系统，如果没有告警智能关联，我们会怎么样？简单举一个例子，如果某个业务在数据层的服务器(假设安装的是redis)有硬件故障，前端业务也没有做好足够的容灾切换，那么该业务的接入层、逻辑层、数据层在用户体验、业务程序层将产生大量的告警，形成告警风暴。
为了解决该问题，腾讯内部有一个ROOT系统，基于业务架构，结合业务数据流访问关系，通过时间相关性、面积权重等算法，将监控告警进行分类、关联，发掘出告警的根源所在。
告警关联的一个基本思路是，越靠近业务后端（逻辑层处于接入层的后端，数据层处于逻辑层的后端）的告警越趋近于故障根源；越靠近基础设施层的告警越趋近于故障根源。
还是刚才所举的例子：监控系统在关联所有告警后，发给运维和研发的告警将是分析后的结论：redis所在服务器硬件故障，导致业务请求量下降xx%，业务整体流量下降XX%。


七、业务容灾调度柔性
我们始终认为：监控系统、运维工具不是万能的。如果要业务可用性不断靠近100%，需要业务侧做很多容灾、调度、柔性的工作。
1、在用户端，为了应对网络环境复杂的情况，腾讯移动类业务采用公司统一的业务接入框架维纳斯【维纳斯(WNS，Wireless Network Service),又名移动连通服务，是一个为APP提供高连通、高可靠、强安全的网络连接通道的服务；它利用QQ、微信海量接入数据来持续优化调度算法，并集成了用户就近接入、腾讯直通车、加密通道透传功能等等，提供了手机端SDK（IOS/Android），业务不必关心网络细节，即可安全与业务后台简单可靠的通讯】。
2、业务接入层：业务接入层大多数是无状态设计（或者是有规则的分号段接入），在运营部署规划的过程中，根据业务规模大小，选择不同程度的容灾，通常有跨交换机、跨机架、跨机房、跨地域容灾。业务全量接入TGW（腾讯云网关）实现负载均衡，避免单个服务器、交换机、机房出现故障时，业务完全瘫痪。
3、业务逻辑层：业务间的逻辑调用都是通过L5组件（名字服务+负载均衡）访问，L5组件基于服务器初始配置信息，通过自适应算法，以两个关键指标请求成功率和请求延时为依据，周期性计算出每个被调服务器的权重，再使用高效的配额算法分配各个主调服务的访问路由，主调服务器上的业务进程通过API来取得这些路由，调用结束时通过API来反馈路由的好与坏。
4、网络调度：主要有同城跨运营商调度和同运营商跨城调度。假设上海电信出口有故障，我们将通过GSLB域名解析指向调度到同城其他运营商的接入集群，实现容灾；
腾讯有几个核心的IDC节点，多个节点之间有专线互联，所以我们也可以将上海电信接入的这部分用户牵引到北京电信或者深圳电信进行接入，实现业务的容灾---这就是同运营商跨城调度。调度的过程，业务完全无感知。
5、柔性：分基础设施层面的柔性和业务逻辑功能上的柔性。
柔性是容灾、调度切换等手段的补充。基础设施层面的柔性，举一个例子：
当运营商网络、专线网络拥塞的时候，我们可以根据业务的服务等级不同启动不通等级的流量控制。
业务功能上的柔性也举一个简单易懂的例子：某个业务如果提供了文字、语音、视频、互动等功能，当网络高负载或者业务整体高负载时，可以通过柔性开关控制关闭调某些高消耗资源的功能和服务。
总结下，监控体系是业务运营体系中非常重要的一个环节，但业务可用性的提高是需要基础设施支撑团队、业务运维团队、业务研发团队一起去通力合作，才能做到更好的。