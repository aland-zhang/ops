初探glusterfs-使用小结FAQ
2016/6/22

一、配置相关
1、快速建立一个卷来提供服务的流程示例
【数据盘分区】
如果分区所在设备已经挂载，要先卸载并删掉现有系统。
yum install lvm2 xfsprogs -y   
pvcreate /dev/sdb
vgcreate vg0 /dev/sdb 
lvcreate -l 100%FREE -n lv01 vg0
mkfs.xfs -f -i size=512 /dev/vg0/lv01 
mkdir /data
cat <<_EOF >>/etc/fstab
UUID=$(blkid /dev/vg0/lv01 |cut -d'"' -f2) /data                   xfs     defaults        0 0
_EOF

mount -a
# df -h |grep data
/dev/mapper/vg0-lv01  16T   33M  16T   1% /data


【配置服务】以在 10.60.200.11 上配置为例
yum install glusterfs-server
service glusterd start
chkconfig glusterd on

【配置集群】
gluster peer probe 10.60.200.12
每台集群节点上建立目录
mkdir /data/gv1/brick1 -p

【提供data域】
创建卷gv0作为主数据域：
# gluster volume create gv0 replica 2 transport tcp \
10.60.200.11:/data/gv1/brick1 \
10.60.200.12:/data/gv1/brick1 

【启动】
# gluster volume start gv1

【查看现状】
# gluster volume info
 
Volume Name: gv1
Type: Replicate
Volume ID: 32b1866c-1743-4dd9-9429-6ecfdfa168a2
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: 10.60.200.11:/data/gv1/brick1
Brick2: 10.60.200.12:/data/gv1/brick1


2、挂载时，可以使用参数来提供备用节点：
mount.gluster -o backupvolfile-server=10.60.200.12 10.60.200.11:/gv1 /mnt/test


3、参数配置
实例：
ovirt的优化做了如下工作：
---
优化后，配置将做如下调整：
Options Reconfigured:
diagnostics.count-fop-hits: on
diagnostics.latency-measurement: on
storage.owner-gid: 36
storage.owner-uid: 36
cluster.server-quorum-type: server
cluster.quorum-type: auto
network.remote-dio: enable
cluster.eager-lock: enable
performance.stat-prefetch: off
performance.io-cache: off
performance.read-ahead: off
performance.quick-read: off
auth.allow: *
user.cifs: enable
nfs.disable: off
performance.readdir-ahead: on
---


---配置卷，以gv1为例：
gluster volume set gv1 diagnostics.count-fop-hits on
gluster volume set gv1 diagnostics.latency-measurement on
gluster volume set gv1 storage.owner-gid 36
gluster volume set gv1 storage.owner-uid 36 
gluster volume set gv1 cluster.server-quorum-type server
gluster volume set gv1 cluster.quorum-type auto
gluster volume set gv1 network.remote-dio enable
gluster volume set gv1 cluster.eager-lock enable
gluster volume set gv1 performance.stat-prefetch off
gluster volume set gv1 performance.io-cache off
gluster volume set gv1 performance.read-ahead off
gluster volume set gv1 performance.quick-read off
gluster volume set gv1 auth.allow \*
gluster volume set gv1 user.cifs enable
gluster volume set gv1 nfs.disable off
---配置卷


二、调整相关
1、扩容
volume add-brick gv_test1 replica 2 10.60.200.21:/data/gv1/brick1 10.60.200.22:/data/gv1/brick1


# gluster volume info
 
Volume Name: gv1
Type: Distributed-Replicate
Volume ID: 32b1866c-1743-4dd9-9429-6ecfdfa168a2
Status: Started
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: 10.60.200.11:/data/gv1/brick1
Brick2: 10.60.200.12:/data/gv1/brick1
Brick3: 10.60.200.21:/data/gv1/brick1
Brick4: 10.60.200.22:/data/gv1/brick1


三、故障相关
1、修复实例：2个副本的复制卷，其中一个挂掉且不能上线后，怎么处理？
1）现状
gluster> volume info
Volume Name: gv1
Type: Replicate
Volume ID: 32b1866c-1743-4dd9-9429-6ecfdfa168a2
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: 10.60.200.11:/data/gv1/brick1
Brick2: 10.60.200.12:/data/gv1/brick1

2）假设
10.60.200.11 挂掉

准备替代的brick：
10.60.200.31:/data/gv1/brick1

3）操作
在 10.60.200.12 上执行 peer probe 来将新的节点加入集群：
gluster> peer probe 10.60.200.31

执行 replace-brick 操作：
gluster> volume replace-brick gv1 10.60.200.11:/data/gv1/brick1 10.60.200.31:/data/gv1/brick1 commit force
volume replace-brick: success: replace-brick commit successful

查看状态：
gluster> volume status
Status of volume: gv1
Gluster process                                         Port    Online  Pid
------------------------------------------------------------------------------
Brick 10.60.200.31:/data/gv1/brick1                     49153   Y       9188
Brick 10.60.200.12:/data/gv1/brick1                     49152   Y       23944
NFS Server on localhost                                 N/A     N       N/A
Self-heal Daemon on localhost                           N/A     Y       14839
NFS Server on 10.60.200.31                              N/A     N       N/A
Self-heal Daemon on 10.60.200.31                        N/A     N       9199
 
Task Status of Volume ovirt-data
------------------------------------------------------------------------------
There are no active volume tasks
 
gluster> volume info
 
Volume Name: gv1
Type: Replicate
Volume ID: 6a2f6489-bb8f-43d9-b46b-6258d95c7571
Status: Started
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: 10.60.200.31:/data/gv1/brick1
Brick2: 10.60.200.12:/data/gv1/brick1


查看 heal 的状态：
gluster> volume heal gv1 info
Brick 10.60.200.31:/data/gv1/brick1
Number of entries: 0

Brick 10.60.200.12:/data/gv1/brick1
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1d04adc8-9921-4e1c-b192-84fba64224db/bd3c9ecf-8100-42db-9db5-038dd95a4d54 - Possibly undergoing heal

/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1a719266-2b0e-4f6a-9586-825e5998c67b/74b85a40-847e-43ca-a74e-a09d5274339f 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/2d7f2fee-c796-4458-a7c2-86cb2e227834/2ef4b3fa-1656-4d15-8bd4-3356eab2d37e - Possibly undergoing heal

/09cb8372-a68d-47dc-962e-70b5225be6bc/dom_md/outbox 
/09cb8372-a68d-47dc-962e-70b5225be6bc/master/vms 
/09cb8372-a68d-47dc-962e-70b5225be6bc/master/tasks 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/48722b60-7ce6-4714-942a-4cace1076c84/cba01bf2-5011-43d2-b724-d6bca8d49477 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/6b4c4ca2-8fab-476c-9321-f23872618837 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/b89e79cf-98bb-44a5-b9a0-33bbff2e4e22 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/560a7b8c-fbfe-4495-b315-e46593b4523e 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/e4c55db2-b33a-4c0d-8bc0-dc0c421ca302 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/e18d032d-c70b-4104-98b5-a3f7fb8d2e27 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/f5e408f3-eded-4bd2-93fd-205cbcd41714 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/2a629e0c-0029-480e-87f4-ac5f417a4448 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/48722b60-7ce6-4714-942a-4cace1076c84/cba01bf2-5011-43d2-b724-d6bca8d49477.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/48722b60-7ce6-4714-942a-4cace1076c84/cba01bf2-5011-43d2-b724-d6bca8d49477.lease 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/6c090128-6a01-4c0e-a3e2-df4a95e30894 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/2d7f2fee-c796-4458-a7c2-86cb2e227834/2ef4b3fa-1656-4d15-8bd4-3356eab2d37e.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/2d7f2fee-c796-4458-a7c2-86cb2e227834/2ef4b3fa-1656-4d15-8bd4-3356eab2d37e.lease 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/bc95b575-99fe-4cdf-a341-aeeed053e2b3 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/bc95b575-99fe-4cdf-a341-aeeed053e2b3.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/bc95b575-99fe-4cdf-a341-aeeed053e2b3.lease 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/6c090128-6a01-4c0e-a3e2-df4a95e30894.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/5f11ddd0-9f74-4845-80ed-d3e5dc081d80/6c090128-6a01-4c0e-a3e2-df4a95e30894.lease 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1d04adc8-9921-4e1c-b192-84fba64224db/bd3c9ecf-8100-42db-9db5-038dd95a4d54.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1d04adc8-9921-4e1c-b192-84fba64224db/bd3c9ecf-8100-42db-9db5-038dd95a4d54.lease 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1a719266-2b0e-4f6a-9586-825e5998c67b/74b85a40-847e-43ca-a74e-a09d5274339f.meta 
/09cb8372-a68d-47dc-962e-70b5225be6bc/images/1a719266-2b0e-4f6a-9586-825e5998c67b/74b85a40-847e-43ca-a74e-a09d5274339f.lease 
Number of entries: 28


等待完成后：
gluster> volume heal gv1 info
Brick 10.60.200.31:/data/gv1/brick1
Number of entries: 0

Brick 10.60.200.12:/data/gv1/brick1
Number of entries: 0




ZYXW、参考
1、[Gluster-users] Replacing a failed brick
https://www.gluster.org/pipermail/gluster-users/2013-August/014000.html


